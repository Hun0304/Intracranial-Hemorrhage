{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1440e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af567355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a2279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ.pop('TF_CONFIG', None)\n",
    "if '.' not in sys.path:\n",
    "  sys.path.insert(0, '.')\n",
    "\n",
    "os.environ['TF_CONFIG'] = json.dumps({\n",
    "    'cluster': {\n",
    "        'worker': ['192.168.0.101:20000', '192.168.0.105:20000']\n",
    "    },\n",
    "    'task': {'type': 'worker', 'index': 0}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74fbdef5-ec5e-4f34-ada4-844349b6a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import segmentation_models as sm\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "710a81fa-a7f0-4437-94ec-a548985a6d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(tfrecords_pattern, rate=0.8, buffer_size=10000):\n",
    "    filenames = tf.io.gfile.glob(tfrecords_pattern)\n",
    "    random.shuffle(filenames)\n",
    "    split_idx = int(len(filenames) * rate)\n",
    "\n",
    "    return filenames[:split_idx], filenames[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5975cdf8-da12-4c09-ab02-65464ae6fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    image = tf.io.decode_png(image, 1, dtype=tf.dtypes.uint16)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1dc12ef5-cc17-4d6e-be0f-bb53c8e3f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(example):\n",
    "    features_description = {\n",
    "        \"filename\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"number\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"sample\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"image_raw\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"mask_raw\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(example, features_description, name=\"nii\")\n",
    "    image = decode_image(example[\"image_raw\"])\n",
    "    mask = tf.cast(decode_image(example[\"mask_raw\"]), dtype=tf.float32)\n",
    "    sample = tf.cast(example[\"sample\"], tf.bool)\n",
    "    \n",
    "    return (image, mask), sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "739cec77-12d3-492d-b48a-03d92d9eefcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filenames):\n",
    "    dataset = tf.data.TFRecordDataset(\n",
    "        filenames,\n",
    "        compression_type=\"GZIP\"\n",
    "    )\n",
    "    dataset = dataset.map(read_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    for n_num, _ in enumerate(dataset):\n",
    "        pass\n",
    "    \n",
    "    dataset = dataset.shuffle(2048, reshuffle_each_iteration=False)\n",
    "\n",
    "    return dataset, n_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6463a7ab-b4e6-41a8-8bc9-bb7b2d750d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(filenames, batch=4, repeat=False):\n",
    "    dataset, n_num = load_dataset(filenames)\n",
    "    dataset = dataset.filter(lambda x, s: s)\n",
    "    dataset = dataset.map(lambda x, s: x)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch)\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    return dataset, n_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "856702c7-164c-4783-bead-da8f8fa9be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(image_batch):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for n in range(len(image_batch)):\n",
    "        ax = plt.subplot(5, 5, n + 1)\n",
    "        plt.imshow(image_batch[n])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "154982f2-dec6-4c14-9e0b-cf75d5585d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # Load Dataset\n",
    "    x_list, y_list = split_train_test(\n",
    "        os.path.join(args.dataset, \"*.tfrecord\"),\n",
    "        rate=args.train_rate\n",
    "    )\n",
    "\n",
    "    x_dataset, x_num = get_dataset(x_list, batch=args.batch, repeat=True)    \n",
    "    y_dataset, _ = get_dataset(y_list, batch=args.batch)\n",
    "    \n",
    "    # Multi Node\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "    \n",
    "    with strategy.scope():\n",
    "        # Build Model\n",
    "        model = sm.Unet(args.backbone, encoder_weights=None, input_shape=(None, None, 1))\n",
    "        \n",
    "        dice_loss = sm.losses.DiceLoss()\n",
    "        focal_loss = sm.losses.BinaryFocalLoss()\n",
    "        totol_loss = dice_loss + (1 * focal_loss)\n",
    "        \n",
    "        model.compile(\n",
    "            keras.optimizers.Adam(args.lr),\n",
    "            loss=totol_loss,\n",
    "            metrics=[sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)],\n",
    "        )\n",
    "            \n",
    "        current_time = strftime('%Y%m%d%H%M%S', gmtime())\n",
    "        logdir = os.path.join(args.logs, f\"{args.name}-{current_time}\")\n",
    "        callbacks = [\n",
    "            keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "            keras.callbacks.ModelCheckpoint(os.path.join(logdir, f\"ICH-{args.name}-\"+\"{epoch}.h5\"), save_weights_only=True, save_best_only=False, mode='min'),\n",
    "            keras.callbacks.ReduceLROnPlateau(),\n",
    "        ]\n",
    "\n",
    "        # Training\n",
    "        history = model.fit(\n",
    "            x_dataset,\n",
    "            epochs=args.epoch,\n",
    "            steps_per_epoch=int(math.ceil(1. * x_num) / args.batch),\n",
    "            callbacks=callbacks,\n",
    "            validation_data=y_dataset\n",
    "        )\n",
    "\n",
    "    print(\"fitted\")\n",
    "    \n",
    "    # Plot training & validation iou_score values\n",
    "    plt.figure(figsize=(30, 5))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['iou_score'])\n",
    "    plt.plot(history.history['val_iou_score'])\n",
    "    plt.title('Model iou_score')\n",
    "    plt.ylabel('iou_score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.savefig(\"training.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "191f7aaa-b926-4fab-885a-1922c3f37631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 16:57:05.744944: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2022-11-07 16:57:06.593364: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1054/1054 [==============================] - 682s 632ms/step - loss: 0.1772 - iou_score: 0.7636 - f1-score: 0.8614 - val_loss: 0.1741 - val_iou_score: 0.7329 - val_f1-score: 0.8428 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "1054/1054 [==============================] - 664s 630ms/step - loss: 0.1045 - iou_score: 0.8264 - f1-score: 0.9038 - val_loss: 0.1152 - val_iou_score: 0.8083 - val_f1-score: 0.8926 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "1054/1054 [==============================] - 664s 630ms/step - loss: 0.0835 - iou_score: 0.8580 - f1-score: 0.9228 - val_loss: 0.0981 - val_iou_score: 0.8347 - val_f1-score: 0.9090 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "1054/1054 [==============================] - 665s 631ms/step - loss: 0.0746 - iou_score: 0.8720 - f1-score: 0.9310 - val_loss: 0.1000 - val_iou_score: 0.8335 - val_f1-score: 0.9083 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "1054/1054 [==============================] - 665s 631ms/step - loss: 0.0667 - iou_score: 0.8845 - f1-score: 0.9382 - val_loss: 0.1059 - val_iou_score: 0.8229 - val_f1-score: 0.9017 - lr: 0.0010\n",
      "Epoch 6/100\n",
      " 352/1054 [=========>....................] - ETA: 7:09 - loss: 0.0593 - iou_score: 0.8965 - f1-score: 0.9451"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--name\",\n",
    "        default=\"ICH420\",\n",
    "        help=\"Training Name\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--backbone\",\n",
    "        default=\"resnet101\",\n",
    "        help=\"Model Backbone\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch\",\n",
    "        default=16,\n",
    "        help=\"Batch Size\",\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epoch\",\n",
    "        default=10,\n",
    "        help=\"Training Epoch\",\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\",\n",
    "        default=0.001,\n",
    "        help=\"Steps per Epoch\",\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        default=os.path.join(os.getcwd(), \"datasets/ICH_420/TFRecords/train\"),\n",
    "        help=\"/path/to/dataset\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_rate\",\n",
    "        default=0.8,\n",
    "        help=\"Use to split dataset to 'train' and 'valid'\",\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--logs\",\n",
    "        default=os.path.join(os.getcwd(), \"logs\"),\n",
    "        help=\"/path/to/logs\"\n",
    "    )\n",
    "    main(parser.parse_args([\n",
    "        \"--name\", \"ICH420\",\n",
    "        \"--backbone\", \"resnet101\",\n",
    "        \"--batch\", \"16\",\n",
    "        \"--epoch\", \"100\",\n",
    "        \"--lr\", \"0.001\",\n",
    "        \"--dataset\", os.path.join(os.getcwd(), \"datasets/ICH_420/TFRecords/train\"),\n",
    "        \"--train_rate\", \"0.8\",\n",
    "        \"--logs\", os.path.join(os.getcwd(), \"logs\")\n",
    "    ]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
