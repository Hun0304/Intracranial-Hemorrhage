{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74fbdef5-ec5e-4f34-ada4-844349b6a3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 16:39:30.427714: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-09 16:39:30.495037: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-09 16:39:30.513500: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import segmentation_models as sm\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "710a81fa-a7f0-4437-94ec-a548985a6d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(tfrecords_pattern, rate=0.8, buffer_size=10000):\n",
    "    filenames = tf.io.gfile.glob(tfrecords_pattern)\n",
    "    random.shuffle(filenames)\n",
    "    split_idx = int(len(filenames) * rate)\n",
    "\n",
    "    return filenames[:split_idx], filenames[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5975cdf8-da12-4c09-ab02-65464ae6fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    image = tf.io.decode_png(image, 1, dtype=tf.dtypes.uint16)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dc12ef5-cc17-4d6e-be0f-bb53c8e3f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(example):\n",
    "    features_description = {\n",
    "        \"filename\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"number\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"sample\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"image_raw\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"mask_raw\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(example, features_description, name=\"nii\")\n",
    "    image = decode_image(example[\"image_raw\"])\n",
    "    mask = tf.cast(decode_image(example[\"mask_raw\"]), dtype=tf.float32)\n",
    "    sample = tf.cast(example[\"sample\"], tf.bool)\n",
    "    \n",
    "    return (image, mask), sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "739cec77-12d3-492d-b48a-03d92d9eefcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filenames):\n",
    "    dataset = tf.data.TFRecordDataset(\n",
    "        filenames,\n",
    "        compression_type=\"GZIP\"\n",
    "    )\n",
    "    dataset = dataset.map(read_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    for n_num, _ in enumerate(dataset):\n",
    "        pass\n",
    "    \n",
    "    dataset = dataset.shuffle(2048, reshuffle_each_iteration=False)\n",
    "\n",
    "    return dataset, n_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6463a7ab-b4e6-41a8-8bc9-bb7b2d750d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(filenames, batch=4, repeat=False):\n",
    "    dataset, n_num = load_dataset(filenames)\n",
    "    dataset = dataset.filter(lambda x, s: s)\n",
    "    dataset = dataset.map(lambda x, s: x)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch)\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    return dataset, n_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "856702c7-164c-4783-bead-da8f8fa9be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(image_batch):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for n in range(len(image_batch)):\n",
    "        ax = plt.subplot(5, 5, n + 1)\n",
    "        plt.imshow(image_batch[n])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "154982f2-dec6-4c14-9e0b-cf75d5585d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # Load Dataset\n",
    "    x_list, y_list = split_train_test(\n",
    "        os.path.join(args.dataset, \"*.tfrecord\"),\n",
    "        rate=args.train_rate\n",
    "    )\n",
    "\n",
    "    x_dataset, x_num = get_dataset(x_list, batch=args.batch, repeat=True)    \n",
    "    y_dataset, _ = get_dataset(y_list, batch=args.batch)\n",
    "    \n",
    "    # Build Model\n",
    "    model = sm.Unet(args.backbone, encoder_weights=None, input_shape=(None, None, 1))\n",
    "    \n",
    "    dice_loss = sm.losses.DiceLoss()\n",
    "    focal_loss = sm.losses.BinaryFocalLoss()\n",
    "    totol_loss = dice_loss + (1 * focal_loss)\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(args.lr),\n",
    "        loss=totol_loss,\n",
    "        metrics=[sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)],\n",
    "    )\n",
    "    \n",
    "    # current_time = strftime('%Y%m%d%H%M%S', gmtime())\n",
    "    current_time = \"20221107165659\"\n",
    "    logdir = os.path.join(args.logs, f\"{args.name}-{current_time}\")\n",
    "    callbacks = [\n",
    "        keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "        keras.callbacks.ModelCheckpoint(os.path.join(logdir, f\"ICH-{args.name}-\"+\"{epoch}.h5\"), save_weights_only=True, save_best_only=False, mode='min'),\n",
    "        keras.callbacks.ReduceLROnPlateau(),\n",
    "    ]\n",
    "\n",
    "    # Training\n",
    "    history = model.fit(\n",
    "        x_dataset,\n",
    "        epochs=args.epoch,\n",
    "        steps_per_epoch=int(math.ceil(1. * x_num) / args.batch),\n",
    "        callbacks=callbacks,\n",
    "        validation_data=y_dataset\n",
    "    )\n",
    "\n",
    "    print(\"fitted\")\n",
    "    \n",
    "    # Plot training & validation iou_score values\n",
    "    plt.figure(figsize=(30, 5))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['iou_score'])\n",
    "    plt.plot(history.history['val_iou_score'])\n",
    "    plt.title('Model iou_score')\n",
    "    plt.ylabel('iou_score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.savefig(\"training.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "191f7aaa-b926-4fab-885a-1922c3f37631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 16:39:31.966344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-09 16:39:31.967940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-09 16:39:31.968012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-09 16:39:31.968311: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-09 16:39:31.968877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-09 16:39:31.968959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-09 16:39:31.969006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-09 16:39:32.540991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-09 16:39:32.541099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-09 16:39:32.541151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-09 16:39:32.541205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22241 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 47\u001b[0m\n\u001b[1;32m     36\u001b[0m parser\u001b[39m.\u001b[39madd_argument(\n\u001b[1;32m     37\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m--train_rate\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m     default\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m,\n\u001b[1;32m     39\u001b[0m     help\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUse to split dataset to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     40\u001b[0m     \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m parser\u001b[39m.\u001b[39madd_argument(\n\u001b[1;32m     43\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m--logs\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     44\u001b[0m     default\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mgetcwd(), \u001b[39m\"\u001b[39m\u001b[39mlogs\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     45\u001b[0m     help\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/path/to/logs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m )\n\u001b[0;32m---> 47\u001b[0m main(parser\u001b[39m.\u001b[39;49mparse_args([\n\u001b[1;32m     48\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m--name\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mICH420\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     49\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m--backbone\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mresnet101\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     50\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m--batch\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m16\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     51\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m--epoch\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m100\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     52\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m--lr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m0.001\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     53\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m--dataset\u001b[39;49m\u001b[39m\"\u001b[39;49m, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(os\u001b[39m.\u001b[39;49mgetcwd(), \u001b[39m\"\u001b[39;49m\u001b[39mdatasets/ICH_420/TFRecords/train\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     54\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m--train_rate\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m0.8\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     55\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m--logs\u001b[39;49m\u001b[39m\"\u001b[39;49m, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(os\u001b[39m.\u001b[39;49mgetcwd(), \u001b[39m\"\u001b[39;49m\u001b[39mlogs\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     56\u001b[0m ]))\n",
      "Cell \u001b[0;32mIn [8], line 34\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     27\u001b[0m callbacks \u001b[39m=\u001b[39m [\n\u001b[1;32m     28\u001b[0m     keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mTensorBoard(log_dir\u001b[39m=\u001b[39mlogdir),\n\u001b[1;32m     29\u001b[0m     keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(logdir, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mICH-\u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{epoch}\u001b[39;00m\u001b[39m.h5\u001b[39m\u001b[39m\"\u001b[39m), save_weights_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, save_best_only\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     30\u001b[0m     keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mReduceLROnPlateau(),\n\u001b[1;32m     31\u001b[0m ]\n\u001b[1;32m     33\u001b[0m \u001b[39m# Training\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     35\u001b[0m     x_dataset,\n\u001b[1;32m     36\u001b[0m     epochs\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mepoch,\n\u001b[1;32m     37\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(math\u001b[39m.\u001b[39;49mceil(\u001b[39m1.\u001b[39;49m \u001b[39m*\u001b[39;49m x_num) \u001b[39m/\u001b[39;49m args\u001b[39m.\u001b[39;49mbatch),\n\u001b[1;32m     38\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m     39\u001b[0m     validation_data\u001b[39m=\u001b[39;49my_dataset,\n\u001b[1;32m     40\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m31\u001b[39;49m\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfitted\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[39m# Plot training & validation iou_score values\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:963\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m   \u001b[39m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[1;32m    962\u001b[0m   initializers \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 963\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwds, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[1;32m    964\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m   \u001b[39m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[1;32m    966\u001b[0m   \u001b[39m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[1;32m    967\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:785\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph \u001b[39m=\u001b[39m lifted_initializer_graph\n\u001b[1;32m    783\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph_deleter \u001b[39m=\u001b[39m FunctionDeleter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph)\n\u001b[1;32m    784\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_stateful_fn \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 785\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateful_fn\u001b[39m.\u001b[39;49m_get_concrete_function_internal_garbage_collected(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    786\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[1;32m    788\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[1;32m    789\u001b[0m   \u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:2523\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2521\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m-> 2523\u001b[0m   graph_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[1;32m   2524\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:2760\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2758\u001b[0m   \u001b[39m# Only get placeholders for arguments, not captures\u001b[39;00m\n\u001b[1;32m   2759\u001b[0m   args, kwargs \u001b[39m=\u001b[39m placeholder_dict[\u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m-> 2760\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[1;32m   2762\u001b[0m graph_capture_container \u001b[39m=\u001b[39m graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_capture_func_lib  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   2763\u001b[0m \u001b[39m# Maintain the list of all captures\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:2670\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2665\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[1;32m   2666\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m   2667\u001b[0m ]\n\u001b[1;32m   2668\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[1;32m   2669\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[0;32m-> 2670\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m   2671\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m   2672\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m   2673\u001b[0m         args,\n\u001b[1;32m   2674\u001b[0m         kwargs,\n\u001b[1;32m   2675\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[1;32m   2676\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[1;32m   2677\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[1;32m   2678\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m   2679\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[1;32m   2680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m   2681\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m   2682\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m   2683\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m   2684\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m   2685\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m   2686\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   2687\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py:1288\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1282\u001b[0m   \u001b[39m# Returning a closed-over tensor does not trigger convert_to_tensor.\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m   func_graph\u001b[39m.\u001b[39moutputs\u001b[39m.\u001b[39mextend(\n\u001b[1;32m   1284\u001b[0m       func_graph\u001b[39m.\u001b[39mcapture(x)\n\u001b[1;32m   1285\u001b[0m       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m flatten(func_graph\u001b[39m.\u001b[39mstructured_outputs)\n\u001b[1;32m   1286\u001b[0m       \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1288\u001b[0m   func_graph\u001b[39m.\u001b[39mvariables \u001b[39m=\u001b[39m variables\n\u001b[1;32m   1290\u001b[0m \u001b[39mif\u001b[39;00m add_control_dependencies:\n\u001b[1;32m   1291\u001b[0m   func_graph\u001b[39m.\u001b[39mcontrol_outputs\u001b[39m.\u001b[39mextend(deps_control_manager\u001b[39m.\u001b[39mops_which_must_run)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/auto_control_deps.py:417\u001b[0m, in \u001b[0;36mAutomaticControlDependencies.__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mif\u001b[39;00m control_flow_util\u001b[39m.\u001b[39mIsInWhileLoop(op):\n\u001b[1;32m    416\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m control_inputs \u001b[39m=\u001b[39m \u001b[39mset\u001b[39;49m()\n\u001b[1;32m    419\u001b[0m \u001b[39mif\u001b[39;00m op\u001b[39m.\u001b[39mtype \u001b[39min\u001b[39;00m MUST_RUN_ORDER_INSENSITIVE_STATEFUL_OPS:\n\u001b[1;32m    420\u001b[0m   \u001b[39m# This will add it to self._independent_ops, but also mark it with an\u001b[39;00m\n\u001b[1;32m    421\u001b[0m   \u001b[39m# attribute.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_independently(op)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--name\",\n",
    "        default=\"ICH420\",\n",
    "        help=\"Training Name\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--backbone\",\n",
    "        default=\"resnet101\",\n",
    "        help=\"Model Backbone\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch\",\n",
    "        default=16,\n",
    "        help=\"Batch Size\",\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epoch\",\n",
    "        default=10,\n",
    "        help=\"Training Epoch\",\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\",\n",
    "        default=0.001,\n",
    "        help=\"Steps per Epoch\",\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        default=os.path.join(os.getcwd(), \"datasets/ICH_420/TFRecords/train\"),\n",
    "        help=\"/path/to/dataset\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_rate\",\n",
    "        default=0.8,\n",
    "        help=\"Use to split dataset to 'train' and 'valid'\",\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--logs\",\n",
    "        default=os.path.join(os.getcwd(), \"logs\"),\n",
    "        help=\"/path/to/logs\"\n",
    "    )\n",
    "    main(parser.parse_args([\n",
    "        \"--name\", \"ICH420\",\n",
    "        \"--backbone\", \"resnet101\",\n",
    "        \"--batch\", \"16\",\n",
    "        \"--epoch\", \"100\",\n",
    "        \"--lr\", \"0.001\",\n",
    "        \"--dataset\", os.path.join(os.getcwd(), \"datasets/ICH_420/TFRecords/train\"),\n",
    "        \"--train_rate\", \"0.8\",\n",
    "        \"--logs\", os.path.join(os.getcwd(), \"logs\")\n",
    "    ]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
